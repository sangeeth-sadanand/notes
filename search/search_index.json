{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"My Personal Notes","text":"<p>Sangeeth Sadanand</p> <ol> <li>Python</li> <li>ML and AI</li> <li></li> </ol>"},{"location":"ML_AI/","title":"Machine Learning and AI","text":"<ol> <li> <p>Statistics</p> </li> <li></li> </ol>"},{"location":"ML_AI/feature_engineering/","title":"Feature engineering","text":""},{"location":"ML_AI/feature_engineering/#handling-missing-value","title":"Handling missing value","text":"<ul> <li>Handling missing value is a key step in cleaning data,</li> <li>Missing values can significantly distort statistical summaries and degrade model performance. </li> <li>The approach depends on the nature, volume, and cause of missing data.</li> </ul>"},{"location":"ML_AI/feature_engineering/#types-of-missing-data","title":"Types of Missing Data","text":"<p>Handling methods should consider why data are missing:</p> <ul> <li>MCAR (Missing Completely at Random): It is unrelated to any variable. These can often be safely removed or imputed.</li> <li>MAR (Missing at Random): It depends on observed data.</li> <li>MNAR (Missing Not at Random): It depends on unobserved factors. These require specialised approaches such as model-based imputations.</li> </ul>"},{"location":"ML_AI/feature_engineering/#technique-to-handle-missing-value","title":"Technique to handle missing value","text":""},{"location":"ML_AI/feature_engineering/#1-deletion-methods","title":"1. Deletion Methods","text":"<ul> <li> <p>Listwise deletion: Removes any rows with missing data in any column, useful when missing data are rare.</p> <pre><code>df.dropna(inplace=True)\n</code></pre> </li> <li> <p>Pairwise deletion: Retains as much data as possible by removing missing values only for relevant variables.</p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#2-simple-imputation","title":"2. Simple Imputation","text":"<p>Replaces missing values with summary statistics of non-missing data:</p> <ul> <li> <p>Mean Imputation: For continuous variables.</p> <pre><code>df['Age'].fillna(df['Age'].mean())\n</code></pre> </li> <li> <p>Median Imputation: Useful when outliers exist.</p> </li> <li> <p>Mode Imputation: For categorical data features.</p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#3-advanced-imputation","title":"3. Advanced Imputation","text":"<ul> <li>KNN Imputation: Estimates missing values based on nearest neighbors in feature space using sklearn.impute.KNNImputer.</li> <li>Iterative Imputation: Uses regression models to predict missing values iteratively (sklearn.impute.IterativeImputer).</li> <li>Multiple Imputation: Creates several imputed datasets and averages results to reduce bias.</li> </ul>"},{"location":"ML_AI/feature_engineering/#4-interpolation","title":"4. Interpolation","text":"<p>Numerical methods estimate values between observed points, e.g. linear or spline interpolation.</p> <pre><code>df['Value'].interpolate(method='linear', inplace=True)\n</code></pre> <p>This is suitable for time series or sequential numerical data.</p>"},{"location":"ML_AI/feature_engineering/#5-encoding-missingness","title":"5. Encoding Missingness","text":"<p>Instead of filling missing data, missingness itself can be treated as a predictive feature by creating a binary flag column indicating the presence of missing values.</p>"},{"location":"ML_AI/feature_engineering/#6-model-based-approaches","title":"6. Model-Based Approaches","text":"<p>Predict missing values using regression models or machine learning algorithms trained on features without missing entries. This approach is common for large datasets where imputations must be contextual.</p>"},{"location":"ML_AI/feature_engineering/#handling-imbalance-dataset","title":"Handling Imbalance Dataset","text":"<ul> <li>When we have a imbalance dataset then out model will be biased towards the category with more imbalance value</li> <li>We can use:<ul> <li>Up sampling</li> <li>Down sampling</li> </ul> </li> </ul>"},{"location":"ML_AI/feature_engineering/#up-sampling","title":"Up sampling","text":"<ul> <li>Interpolate new data for the categories with the less frequency sample (SMOTE, ADASYN, SIMO)</li> <li>This will add samples to the dataset to reduce biasness in the data</li> </ul>"},{"location":"ML_AI/feature_engineering/#down-sampling","title":"Down sampling","text":"<ul> <li>Here we remove the data from the biased category </li> <li>The number of samples are reduced </li> <li>This is not recommended if the dataset us small or huge count of the majority and minority is observed</li> </ul>"},{"location":"ML_AI/feature_engineering/#smote","title":"SMOTE","text":"<ul> <li>SMOTE stands for Synthetic Minority Over-sampling Technique, and it generates synthetic samples for the minority class to balance the dataset.</li> <li>Instead of duplicating minority samples, SMOTE creates synthetic examples by interpolating between existing ones. </li> <li>This helps reduce overfitting and improves generalization.</li> </ul>"},{"location":"ML_AI/feature_engineering/#adasyn","title":"ADASYN","text":"<ul> <li> <p>ADASYN (Adaptive Synthetic Sampling) is another powerful technique for handling imbalanced datasets. </p> </li> <li> <p>Focuses on samples that are harder to learn (i.e., near decision boundaries), making the classifier more robust.</p> </li> <li> <p>It generates more synthetic samples for these difficult cases and fewer for easier ones.</p> </li> <li> <p>The total number of synthetic samples is based on a desired imbalance ratio, not necessarily full balance.</p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#handling-outlier","title":"Handling Outlier","text":"<ul> <li>To handle outliers effectively, you can detect them using statistical methods like Z-score or IQR, and then choose to remove, cap, or transform them based on your analysis goals.</li> </ul>"},{"location":"ML_AI/feature_engineering/#detection-techniques","title":"\ud83d\udd0d Detection Techniques","text":"<ol> <li>Z-score Method<ul> <li>Calculates how many standard deviations a data point is from the mean.</li> <li>Typically, values with a Z-score &gt; 3 or &lt; -3 are considered outliers.</li> <li>Best for normally distributed data.</li> </ul> </li> <li>Interquartile Range (IQR)<ul> <li>Based on the spread between the 25th (Q1) and 75th (Q3) percentiles.</li> <li>Outliers fall below Q1 - 1.5\u00d7IQR or above Q3 + 1.5\u00d7IQR.</li> <li>Robust against non-normal distributions.</li> </ul> </li> <li>Isolation Forest<ul> <li>An ensemble method that isolates anomalies by randomly selecting features and split values.</li> <li>Effective for high-dimensional datasets.</li> </ul> </li> <li>Local Outlier Factor (LOF)<ul> <li>Measures the local density deviation of a data point compared to its neighbors.</li> <li>Useful for detecting outliers in clusters.</li> </ul> </li> <li>Mahalanobis Distance<ul> <li>Measures distance from the mean considering correlations between variables.</li> <li>Suitable for multivariate outlier detection.</li> </ul> </li> </ol>"},{"location":"ML_AI/feature_engineering/#handling-strategies","title":"\ud83d\udee0\ufe0f Handling Strategies","text":"<ul> <li>Remove Outliers<ul> <li>Safest when outliers are due to data entry errors or irrelevant anomalies.</li> <li>Risk: Loss of valuable information if outliers are meaningful.</li> </ul> </li> <li>Cap or Winsorize<ul> <li>Replace extreme values with a threshold (e.g., 5th and 95th percentiles).</li> <li>Preserves data size while reducing impact.</li> </ul> </li> <li>Transform Data<ul> <li>Apply log, square root, or Box-Cox transformations to reduce skewness.</li> <li>Helps normalize distributions and reduce outlier influence.</li> </ul> </li> <li>Impute Values<ul> <li>Replace outliers with mean, median, or predicted values.</li> <li>Useful when data integrity must be preserved.</li> </ul> </li> <li>Use Robust Models<ul> <li>Some algorithms (e.g., tree-based models) are less sensitive to outliers.</li> <li>Consider switching to models that handle noise better.</li> </ul> </li> </ul>"},{"location":"ML_AI/feature_engineering/#when-to-keep-outliers","title":"\u26a0\ufe0f When to Keep Outliers","text":"<p>Outliers may represent rare but important phenomena (e.g., fraud detection, medical anomalies). Always assess their context before removal.</p> Method Best For Assumptions Pros Cons Z-Score Normal distributions Data is Gaussian Simple, fast, interpretable Sensitive to mean/variance, not robust IQR Skewed or non-normal data None Robust to outliers, easy to compute Ignores multivariate structure Isolation Forest High-dimensional, large datasets None Fast, works well with many features May misclassify rare but valid points LOF Clustered or local anomalies Density-based Captures local structure, unsupervised Sensitive to <code>n_neighbors</code> parameter Mahalanobis Multivariate Gaussian data Linear relationships Accounts for feature correlation Requires invertible covariance matrix"},{"location":"ML_AI/feature_engineering/#data-encoding","title":"Data Encoding","text":"<ul> <li> <p>Data encoding transforms categorical variables into numerical formats so machine learning models can process them effectively. </p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#types-of-categorical-data","title":"Types of Categorical Data","text":"<ul> <li>Binary: Two categories (e.g., Yes/No)</li> <li>Ordinal: Ordered categories (e.g., Low, Medium, High)</li> <li>Nominal: Unordered categories (e.g., Red, Blue, Green)</li> </ul>"},{"location":"ML_AI/feature_engineering/#common-encoding-techniques","title":"Common Encoding Techniques","text":"Technique Best For Description Pros Cons One-Hot Encoding Nominal Creates binary columns for each category. Preserves category independence Increases dimensionality Label Encoding Ordinal Assigns each category a unique integer. Simple, fast Imposes order where none exists Ordinal Encoding Ordinal Maps categories to ordered integers. Retains order Assumes equal spacing Target Encoding Nominal/Ordinal Replaces category with mean of target variable. Captures relationship with target Risk of data leakage Frequency Encoding Nominal Replaces category with its frequency count. Simple, preserves distribution May lose semantic meaning Binary Encoding High-cardinality Converts categories to binary digits and splits into columns. Reduces dimensionality Less interpretable"},{"location":"ML_AI/statistics/","title":"Statistics","text":"<p>Statistics is the branch of mathematics that deals with the collection, organization, analysis, interpretation, and presentation of data.  </p>"},{"location":"ML_AI/statistics/#introduction","title":"Introduction","text":""},{"location":"ML_AI/statistics/#role-of-statistics-in-ml-and-data-science","title":"Role of Statistics in ML and Data Science","text":"<p>Statistics plays a vital role in ML and Data Science by enabling:</p> <ul> <li>Data Summarization: Condenses large datasets into interpretable summaries (mean, median, mode, variance, etc.).</li> <li>Understanding Relationships: Identifies patterns and correlations between variables.</li> <li>Probability Modeling: Helps model uncertainty and make predictions.</li> <li>Hypothesis Testing: Assesses assumptions and validates models.</li> <li>Feature Selection and Engineering: Guides which variables to include in models.</li> <li>Model Evaluation: Statistical metrics (MSE, RMSE, R\u00b2) are used to evaluate predictive performance.</li> <li>Decision Making: Facilitates informed decisions based on data-driven insights.</li> </ul>"},{"location":"ML_AI/statistics/#descriptive-vs-inferential-statistics","title":"Descriptive vs. Inferential Statistics","text":""},{"location":"ML_AI/statistics/#descriptive-statistics","title":"Descriptive Statistics","text":"<ul> <li>Purpose: Summarizes and describes the main features of a dataset.</li> <li>Techniques:<ul> <li>Central Tendency: Mean, Median, Mode</li> <li>Dispersion: Range, Variance, Standard Deviation, IQR</li> <li>Visualization: Histograms, Boxplots, Bar Charts, Scatter Plots</li> </ul> </li> <li>Use Case: Quick understanding of dataset characteristics.</li> </ul>"},{"location":"ML_AI/statistics/#inferential-statistics","title":"Inferential Statistics","text":"<ul> <li>Purpose: Makes predictions or inferences about a population based on a sample.</li> <li>Techniques:<ul> <li>Hypothesis Testing (t-test, chi-square test, ANOVA)</li> <li>Confidence Intervals</li> <li>Regression Analysis</li> <li>Correlation Analysis</li> </ul> </li> <li>Use Case: Generalizing findings beyond the observed data.</li> </ul>"},{"location":"ML_AI/statistics/#population-vs-sample","title":"Population vs. Sample","text":"Term Definition Example Population The entire set of items or observations of interest All users of a social media platform Sample A subset of the population used to estimate population properties 1,000 users randomly selected for survey <p>Key Points:</p> <ul> <li>Samples are used because collecting data from the entire population is often impractical.</li> <li>Statistical inference allows conclusions about the population from the sample.</li> <li>Bias must be minimized to ensure sample represents population accurately.</li> </ul>"},{"location":"ML_AI/statistics/#variables-and-types-of-variables","title":"Variables and Types of Variables","text":"<p>A variable is a characteristic, attribute, or property that can take on different values for different individuals or observations. Variables are fundamental because they represent the data we collect, analyze, and model.  </p>"},{"location":"ML_AI/statistics/#types-of-variables","title":"Types of Variables","text":"<p>Variables are broadly classified into Quantitative (Numerical) and Qualitative (Categorical) variables.  </p>"},{"location":"ML_AI/statistics/#1-quantitative-numerical-variables","title":"1. Quantitative (Numerical) Variables","text":"<ul> <li>Represent measurable quantities.</li> <li>Can be counted or measured.</li> <li>Subtypes:<ol> <li>Discrete Variables: Take countable values, usually integers.<ul> <li>Example: Number of students in a class, number of cars in a parking lot.</li> </ul> </li> <li>Continuous Variables: Can take any value within a range.<ul> <li>Example: Height, Weight, Temperature, Age (measured precisely).</li> </ul> </li> </ol> </li> </ul>"},{"location":"ML_AI/statistics/#2-qualitative-categorical-variables","title":"2. Qualitative (Categorical) Variables","text":"<ul> <li>Represent categories or labels rather than numeric values.</li> <li>Subtypes:<ol> <li>Nominal Variables<ul> <li>Categories without order.</li> <li>Example: Gender, Blood Type, Color.</li> </ul> </li> <li>Ordinal Variables<ul> <li>Categories with a meaningful order, but intervals between categories are not uniform.</li> </ul> </li> <li>Example: Rating scales (Poor, Average, Good, Excellent), Education Level.</li> </ol> </li> </ul>"},{"location":"ML_AI/statistics/#3-other-variable-types","title":"3. Other Variable Types","text":"<ul> <li>Binary / Dichotomous Variables<ul> <li>Only two categories (0 or 1, Yes or No).</li> <li>Example: Has disease (Yes/No), Pass/Fail.</li> </ul> </li> <li>Interval Variables<ul> <li>Numeric variables with equal intervals but no true zero.</li> <li>Example: Temperature in Celsius or Fahrenheit.</li> </ul> </li> <li>Ratio Variables<ul> <li>Numeric variables with equal intervals and a true zero.</li> <li>Example: Height, Weight, Age, Income.</li> </ul> </li> </ul>"},{"location":"ML_AI/statistics/#summary","title":"Summary","text":"Variable Type Sub-type Definition Examples Quantitative / Numerical Measurable quantities Height, Weight, Age Discrete Countable, finite Number of students, Cars Continuous Any value in a range Temperature, Weight Qualitative / Categorical Categories or labels Gender, Blood Type Nominal No order Color, City, Marital Status Ordinal Ordered categories Ratings, Education Level Binary / Dichotomous Two categories only Yes/No, True/False Interval Numeric, equal intervals, no true zero Temperature (\u00b0C, \u00b0F), IQ Ratio Numeric, equal intervals, true zero Age, Salary, Weight"},{"location":"ML_AI/statistics/#scale-of-measurement","title":"Scale of Measurement","text":"<p>The scale of measurement defines how data values are quantified, categorized, and interpreted. It determines the types of statistical analyses that are appropriate for the data.</p>"},{"location":"ML_AI/statistics/#types-of-scales","title":"Types of Scales","text":"<ol> <li> <p>Nominal Scale</p> <ul> <li>Definition: Data are categorized into distinct groups with no natural order.</li> <li>Operations: Counting, mode</li> <li>Examples: Gender, Blood Type, Eye Color</li> </ul> </li> <li> <p>Ordinal Scale</p> <ul> <li>Definition: Data are categorized with a meaningful order, but the intervals between categories are not necessarily equal.</li> <li>Operations: Ranking, median, percentiles</li> <li>Examples: Customer satisfaction (Poor, Fair, Good, Excellent), Education Level (High School, Bachelor\u2019s, Master\u2019s)</li> </ul> </li> <li> <p>Interval Scale</p> <ul> <li>Definition: Numeric data with equal intervals between values, but no true zero point.</li> <li>Operations: Addition, subtraction, mean, standard deviation</li> <li>Examples: Temperature in Celsius/Fahrenheit, IQ scores</li> <li>Note: Ratios are not meaningful (e.g., 20\u00b0C is not \u201ctwice as hot\u201d as 10\u00b0C).</li> </ul> </li> <li> <p>Ratio Scale</p> <ul> <li>Definition: Numeric data with equal intervals and a true zero point.</li> <li>Operations: All arithmetic operations, including ratios</li> <li>Examples: Height, Weight, Age, Income</li> <li>Note: Zero means absence of the property, so statements like \u201ctwice as much\u201d are meaningful.</li> </ul> </li> </ol>"},{"location":"ML_AI/statistics/#sampling-techniques","title":"Sampling Techniques","text":""},{"location":"ML_AI/statistics/#probability-sampling","title":"Probability Sampling","text":"<p>In probability sampling, each member of the population has a known, non-zero chance of being selected. This allows for statistical inference.</p>"},{"location":"ML_AI/statistics/#simple-random-sampling-srs","title":"Simple Random Sampling (SRS)","text":"<ul> <li>Every individual has an equal chance of being selected.</li> <li>Methods: Random number generators, lottery method.</li> <li>Example: Selecting 100 students randomly from a university.</li> </ul>"},{"location":"ML_AI/statistics/#systematic-sampling","title":"Systematic Sampling","text":"<ul> <li>Select every k-th member from a population list after a random start.</li> <li>Example: Choosing every 10th customer entering a store.</li> </ul>"},{"location":"ML_AI/statistics/#stratified-sampling","title":"Stratified Sampling","text":"<ul> <li>Population divided into homogeneous subgroups (strata), and a random sample is taken from each stratum.</li> <li>Ensures representation from all subgroups.</li> <li>Example: Sampling students by department to represent each department proportionally.</li> </ul>"},{"location":"ML_AI/statistics/#cluster-sampling","title":"Cluster Sampling","text":"<ul> <li>Population divided into clusters (often naturally occurring groups), and entire clusters are randomly selected.</li> <li>More practical for large, spread-out populations.</li> <li>Example: Selecting certain schools from a city and surveying all students in those schools.</li> </ul>"},{"location":"ML_AI/statistics/#multistage-sampling","title":"Multistage Sampling","text":"<ul> <li>Combines several sampling methods in stages.</li> <li>Example: Randomly selecting districts \u2192 then schools \u2192 then students within schools.</li> </ul>"},{"location":"ML_AI/statistics/#non-probability-sampling","title":"Non-Probability Sampling","text":"<p>In non-probability sampling, not all members have a known or equal chance of being selected. Easier to implement but may introduce bias.</p>"},{"location":"ML_AI/statistics/#convenience-sampling","title":"Convenience Sampling","text":"<ul> <li>Selects members easily accessible.</li> <li>Example: Surveying people passing by a mall.</li> <li>Limitation: May not be representative.</li> </ul>"},{"location":"ML_AI/statistics/#judgmental-purposive-sampling","title":"Judgmental / Purposive Sampling","text":"<ul> <li>Members are chosen based on expert judgment.</li> <li>Example: Selecting key industry experts for an interview.</li> </ul>"},{"location":"ML_AI/statistics/#quota-sampling","title":"Quota Sampling","text":"<ul> <li>Ensures certain characteristics are represented, but selection is non-random.</li> <li>Example: Ensuring 50% male and 50% female respondents in a survey.</li> </ul>"},{"location":"ML_AI/statistics/#snowball-sampling","title":"Snowball Sampling","text":"<ul> <li>Existing participants recruit future participants from their acquaintances.</li> <li>Useful for hard-to-reach populations.</li> <li>Example: Studying a rare disease or underground community.</li> </ul>"},{"location":"ML_AI/statistics/#summary-table","title":"Summary Table","text":"Sampling Type Method Key Feature Example Probability Simple Random Equal chance Random students Systematic Every k-th selected Every 10th customer Stratified Proportional representation Students by department Cluster Randomly selected clusters Schools in a city Multistage Multiple stages District \u2192 School \u2192 Student Non-Probability Convenience Easily accessible Mall visitors Judgmental / Purposive Expert-selected Industry experts Quota Fixed quotas Gender-balanced survey Snowball Participant referrals Rare disease study"},{"location":"ML_AI/statistics/#data-summarization-and-descriptive-statistics","title":"Data Summarization and Descriptive Statistics","text":""},{"location":"ML_AI/statistics/#measures-of-central-tendency","title":"Measures of Central Tendency","text":"<p>Measures of central tendency describe the central or typical value of a dataset.</p>"},{"location":"ML_AI/statistics/#mean","title":"Mean","text":"<ul> <li>Definition: Arithmetic average of all observations.</li> <li>Formula:  </li> </ul> \\[ \\text{Mean} = \\frac{\\sum x_i}{n} \\] <ul> <li>Advantages: Uses all data points; mathematically convenient.</li> <li>Limitations: Highly sensitive to outliers.</li> <li>Use Case: Normally distributed numerical data.</li> </ul>"},{"location":"ML_AI/statistics/#median","title":"Median","text":"<ul> <li>Definition: Middle value when data is ordered.</li> <li>Key Feature: Robust to outliers.</li> <li>Use Case: Skewed distributions (e.g., income, house prices).</li> </ul>"},{"location":"ML_AI/statistics/#mode","title":"Mode","text":"<ul> <li>Definition: Most frequently occurring value.</li> <li>Key Feature: Can be used for categorical data.</li> <li>Use Case: Identifying common categories or values.</li> </ul>"},{"location":"ML_AI/statistics/#measures-of-dispersion","title":"Measures of Dispersion","text":"<p>Measures of dispersion describe the spread or variability of data.</p>"},{"location":"ML_AI/statistics/#range","title":"Range","text":"<ul> <li>Definition: Difference between maximum and minimum values.</li> <li>Formula: </li> </ul> \\[ \\text{Range} = \\text{Max} - \\text{Min} \\] <ul> <li>Limitation: Highly sensitive to extreme values.</li> </ul>"},{"location":"ML_AI/statistics/#variance","title":"Variance","text":"<ul> <li>Definition: Average of squared deviations from the mean.</li> <li>Formula:  </li> </ul> \\[ \\sigma^2 = \\frac{\\sum (x_i - \\mu)^2}{n} \\] <ul> <li>Interpretation: Measures overall variability.</li> </ul>"},{"location":"ML_AI/statistics/#standard-deviation","title":"Standard Deviation","text":"<ul> <li>Definition: Square root of variance.</li> <li>Advantage: Expressed in the same units as data.</li> <li>Interpretation: Typical distance of data points from the mean.</li> </ul>"},{"location":"ML_AI/statistics/#interquartile-range-iqr","title":"Interquartile Range (IQR)","text":"<ul> <li>Definition: Spread of the middle 50% of data.</li> <li>Formula:  </li> </ul> \\[ \\text{IQR} = Q_3 - Q_1 \\] <ul> <li>Advantage: Resistant to outliers.</li> <li>Use Case: Boxplots and skewed data analysis.</li> </ul>"},{"location":"ML_AI/statistics/#skewness-and-kurtosis","title":"Skewness and Kurtosis","text":""},{"location":"ML_AI/statistics/#skewness","title":"Skewness","text":"<ul> <li>Definition: Measures the asymmetry of a distribution.</li> <li>Types:<ul> <li>Positive Skew (Right-skewed): Long right tail, Mean &gt; Median.</li> <li>Negative Skew (Left-skewed): Long left tail, Mean &lt; Median.</li> <li>Zero Skew: Symmetrical distribution.</li> </ul> </li> </ul>"},{"location":"ML_AI/statistics/#kurtosis","title":"Kurtosis","text":"<ul> <li>Definition: Measures the tailedness or peakedness of a distribution.</li> <li>Types:<ul> <li>Leptokurtic: Heavy tails, sharp peak.</li> <li>Mesokurtic: Normal distribution.</li> <li>Platykurtic: Light tails, flat peak.</li> </ul> </li> </ul>"},{"location":"ML_AI/statistics/#five-number-summary","title":"Five-Number Summary","text":"<p>The five-number summary provides a concise description of a dataset.</p>"},{"location":"ML_AI/statistics/#components","title":"Components:","text":"<ol> <li>Minimum</li> <li>First Quartile (Q1) \u2013 25th percentile</li> <li>Median (Q2) \u2013 50th percentile</li> <li>Third Quartile (Q3) \u2013 75th percentile</li> <li>Maximum</li> </ol> <p>Commonly visualized using a boxplot.</p>"},{"location":"ML_AI/statistics/#3-probability-fundamentals","title":"3. Probability Fundamentals","text":""},{"location":"ML_AI/statistics/#13-basic-probability-concepts","title":"13. Basic Probability Concepts","text":"<p>Probability quantifies the likelihood of an event occurring and is fundamental to uncertainty modeling in statistics, machine learning, and data science.</p> <ul> <li>Probability Value Range:   [   0 \\leq P(A) \\leq 1   ]</li> <li>Interpretation:</li> <li>0 \u2192 Impossible event</li> <li>1 \u2192 Certain event</li> <li>Classical Probability:   [   P(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}   ]</li> </ul>"},{"location":"ML_AI/statistics/#14-sample-space-and-events","title":"14. Sample Space and Events","text":""},{"location":"ML_AI/statistics/#141-sample-space-s","title":"14.1 Sample Space (S)","text":"<ul> <li>Definition: The set of all possible outcomes of a random experiment.</li> <li>Example:</li> <li>Tossing a coin:     [     S = {H, T}     ]</li> <li>Rolling a die:     [     S = {1, 2, 3, 4, 5, 6}     ]</li> </ul>"},{"location":"ML_AI/statistics/#142-events","title":"14.2 Events","text":"<ul> <li>Definition: Any subset of the sample space.</li> <li>Types of Events:</li> <li>Simple Event: Single outcome (e.g., rolling a 3).</li> <li>Compound Event: Multiple outcomes (e.g., rolling an even number).</li> <li>Impossible Event: Cannot occur (probability = 0).</li> <li>Certain Event: Always occurs (probability = 1).</li> </ul>"},{"location":"ML_AI/statistics/#15-conditional-probability","title":"15. Conditional Probability","text":"<ul> <li>Definition: Probability of an event occurring given that another event has already occurred.</li> <li>Notation:   [   P(A \\mid B)   ]</li> <li>Formula:   [   P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) \\neq 0   ]</li> <li>Interpretation: Updates the probability of event A using information that B has occurred.</li> <li>Use Case: Medical diagnosis, fraud detection, recommendation systems.</li> </ul>"},{"location":"ML_AI/statistics/#16-bayes-theorem","title":"16. Bayes\u2019 Theorem","text":"<p>Bayes\u2019 theorem describes how to update probabilities based on new evidence.</p>"},{"location":"ML_AI/statistics/#formula","title":"Formula:","text":"\\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)} \\]"},{"location":"ML_AI/statistics/#components_1","title":"Components:","text":"<ul> <li>P(A): Prior probability</li> <li>P(B | A): Likelihood</li> <li>P(B): Evidence (normalizing constant)</li> <li>P(A | B): Posterior probability</li> </ul>"},{"location":"ML_AI/statistics/#importance-in-ml-data-science","title":"Importance in ML &amp; Data Science:","text":"<ul> <li>Foundation of Bayesian inference.</li> <li>Used in Naive Bayes classifiers, spam filtering, and probabilistic modeling.</li> </ul>"},{"location":"ML_AI/statistics/#17-independence-and-mutual-exclusivity","title":"17. Independence and Mutual Exclusivity","text":""},{"location":"ML_AI/statistics/#171-independent-events","title":"17.1 Independent Events","text":"<ul> <li>Two events A and B are independent if occurrence of one does not affect the probability of the other.</li> <li>Condition:   [   P(A \\cap B) = P(A) \\cdot P(B)   ]</li> <li>Example: Tossing two separate coins.</li> </ul>"},{"location":"ML_AI/statistics/#172-mutually-exclusive-events","title":"17.2 Mutually Exclusive Events","text":"<ul> <li>Two events are mutually exclusive if they cannot occur simultaneously.</li> <li>Condition:   [   P(A \\cap B) = 0   ]</li> <li>Example: Rolling a die and getting both 1 and 6 in a single roll.</li> </ul>"},{"location":"ML_AI/statistics/#173-key-difference","title":"17.3 Key Difference","text":"Aspect Independent Events Mutually Exclusive Events Can occur together Yes No Intersection Non-zero Zero Conditional probability Unchanged Zero <p>Important Note: - Mutually exclusive events cannot be independent unless one has zero probability.</p>"},{"location":"ML_AI/statistics/#4-random-variables-and-probability-distributions","title":"4. Random Variables and Probability Distributions","text":"<ul> <li>Discrete vs. continuous random variables</li> <li>Probability mass and density functions</li> <li>Expectation and variance</li> <li>Common discrete distributions (Bernoulli, Binomial, Poisson)</li> <li>Common continuous distributions (Uniform, Normal, Exponential)</li> </ul>"},{"location":"ML_AI/statistics/#5-normal-distribution-and-central-limit-theorem","title":"5. Normal Distribution and Central Limit Theorem","text":"<ul> <li>Properties of the normal distribution</li> <li>Standard normal distribution and z-scores</li> <li>Central Limit Theorem</li> <li>Practical implications for ML</li> </ul>"},{"location":"ML_AI/statistics/#6-data-visualization-for-statistical-analysis","title":"6. Data Visualization for Statistical Analysis","text":"<ul> <li>Histograms and density plots</li> <li>Box plots and violin plots</li> <li>Scatter plots and pair plots</li> <li>Correlation heatmaps</li> <li>Visualization pitfalls</li> </ul>"},{"location":"ML_AI/statistics/#7-correlation-and-covariance","title":"7. Correlation and Covariance","text":"<ul> <li>Covariance and interpretation</li> <li>Pearson correlation coefficient</li> <li>Spearman rank correlation</li> <li>Multicollinearity</li> <li>Correlation vs. causation</li> </ul>"},{"location":"ML_AI/statistics/#8-statistical-inference","title":"8. Statistical Inference","text":"<ul> <li>Sampling methods</li> <li>Point estimation</li> <li>Confidence intervals</li> <li>Bias and variance trade-off</li> <li>Estimation errors</li> </ul>"},{"location":"ML_AI/statistics/#9-hypothesis-testing","title":"9. Hypothesis Testing","text":"<ul> <li>Null and alternative hypotheses</li> <li>Type I and Type II errors</li> <li>p-values and significance levels</li> <li>Common tests (z-test, t-test, chi-square test)</li> <li>A/B testing fundamentals</li> </ul>"},{"location":"ML_AI/statistics/#10-statistical-assumptions-in-machine-learning","title":"10. Statistical Assumptions in Machine Learning","text":"<ul> <li>Linearity</li> <li>Normality</li> <li>Homoscedasticity</li> <li>Independence of observations</li> <li>Identifying assumption violations</li> </ul>"},{"location":"ML_AI/statistics/#11-outliers-and-data-quality","title":"11. Outliers and Data Quality","text":"<ul> <li>Detecting outliers (IQR, z-score)</li> <li>Impact of outliers on ML models</li> <li>Handling missing data</li> <li>Data normalization and standardization</li> </ul>"},{"location":"ML_AI/statistics/#12-basic-statistical-concepts-in-ml-algorithms","title":"12. Basic Statistical Concepts in ML Algorithms","text":"<ul> <li>Loss functions and statistical meaning</li> <li>Maximum likelihood estimation</li> <li>Bias-variance decomposition</li> <li>Overfitting and underfitting from a statistical view</li> </ul>"},{"location":"ML_AI/statistics/#13-practical-statistics-with-python-optional","title":"13. Practical Statistics with Python (Optional)","text":"<ul> <li>NumPy and Pandas for statistics</li> <li>SciPy for probability and hypothesis testing</li> <li>Visualization with Matplotlib and Seaborn</li> <li>Real-world ML dataset examples</li> </ul>"}]}