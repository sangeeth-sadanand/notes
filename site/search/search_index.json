{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"MyPersonalNotes/","title":"My Personal Notes","text":"<p>Sangeeth Sadanand</p> <ol> <li>Python</li> <li>ML and AI</li> <li></li> </ol>"},{"location":"ML_AI/","title":"Machine Learning and AI","text":"<ol> <li>Feature Engineering</li> <li></li> </ol>"},{"location":"ML_AI/feature_engineering/","title":"Feature engineering","text":""},{"location":"ML_AI/feature_engineering/#handling-missing-value","title":"Handling missing value","text":"<ul> <li>Handling missing value is a key step in cleaning data,</li> <li>Missing values can significantly distort statistical summaries and degrade model performance. </li> <li>The approach depends on the nature, volume, and cause of missing data.</li> </ul>"},{"location":"ML_AI/feature_engineering/#types-of-missing-data","title":"Types of Missing Data","text":"<p>Handling methods should consider why data are missing:</p> <ul> <li>MCAR (Missing Completely at Random): It is unrelated to any variable. These can often be safely removed or imputed.</li> <li>MAR (Missing at Random): It depends on observed data.</li> <li>MNAR (Missing Not at Random): It depends on unobserved factors. These require specialised approaches such as model-based imputations.</li> </ul>"},{"location":"ML_AI/feature_engineering/#technique-to-handle-missing-value","title":"Technique to handle missing value","text":""},{"location":"ML_AI/feature_engineering/#1-deletion-methods","title":"1. Deletion Methods","text":"<ul> <li> <p>Listwise deletion: Removes any rows with missing data in any column, useful when missing data are rare.</p> <p><code>python df.dropna(inplace=True)</code></p> </li> <li> <p>Pairwise deletion: Retains as much data as possible by removing missing values only for relevant variables.</p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#2-simple-imputation","title":"2. Simple Imputation","text":"<p>Replaces missing values with summary statistics of non-missing data:</p> <ul> <li> <p>Mean Imputation: For continuous variables.</p> <p><code>python df['Age'].fillna(df['Age'].mean())</code></p> </li> <li> <p>Median Imputation: Useful when outliers exist.</p> </li> <li> <p>Mode Imputation: For categorical data features.</p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#3-advanced-imputation","title":"3. Advanced Imputation","text":"<ul> <li>KNN Imputation: Estimates missing values based on nearest neighbors in feature space using sklearn.impute.KNNImputer.</li> <li>Iterative Imputation: Uses regression models to predict missing values iteratively (sklearn.impute.IterativeImputer).</li> <li>Multiple Imputation: Creates several imputed datasets and averages results to reduce bias.</li> </ul>"},{"location":"ML_AI/feature_engineering/#4-interpolation","title":"4. Interpolation","text":"<p>Numerical methods estimate values between observed points, e.g. linear or spline interpolation.</p> <pre><code>df['Value'].interpolate(method='linear', inplace=True)\n</code></pre> <p>This is suitable for time series or sequential numerical data.</p>"},{"location":"ML_AI/feature_engineering/#5-encoding-missingness","title":"5. Encoding Missingness","text":"<p>Instead of filling missing data, missingness itself can be treated as a predictive feature by creating a binary flag column indicating the presence of missing values.</p>"},{"location":"ML_AI/feature_engineering/#6-model-based-approaches","title":"6. Model-Based Approaches","text":"<p>Predict missing values using regression models or machine learning algorithms trained on features without missing entries. This approach is common for large datasets where imputations must be contextual.</p>"},{"location":"ML_AI/feature_engineering/#handling-imbalance-dataset","title":"Handling Imbalance Dataset","text":"<ul> <li>When we have a imbalance dataset then out model will be biased towards the category with more imbalance value</li> <li>We can use:<ul> <li>Up sampling</li> <li>Down sampling</li> </ul> </li> </ul>"},{"location":"ML_AI/feature_engineering/#up-sampling","title":"Up sampling","text":"<ul> <li>Interpolate new data for the categories with the less frequency sample (SMOTE, ADASYN, SIMO)</li> <li>This will add samples to the dataset to reduce biasness in the data</li> </ul>"},{"location":"ML_AI/feature_engineering/#down-sampling","title":"Down sampling","text":"<ul> <li>Here we remove the data from the biased category </li> <li>The number of samples are reduced </li> <li>This is not recommended if the dataset us small or huge count of the majority and minority is observed</li> </ul>"},{"location":"ML_AI/feature_engineering/#smote","title":"SMOTE","text":"<ul> <li>SMOTE stands for Synthetic Minority Over-sampling Technique, and it generates synthetic samples for the minority class to balance the dataset.</li> <li>Instead of duplicating minority samples, SMOTE creates synthetic examples by interpolating between existing ones. </li> <li>This helps reduce overfitting and improves generalization.</li> </ul>"},{"location":"ML_AI/feature_engineering/#adasyn","title":"ADASYN","text":"<ul> <li> <p>ADASYN (Adaptive Synthetic Sampling) is another powerful technique for handling imbalanced datasets. </p> </li> <li> <p>Focuses on samples that are harder to learn (i.e., near decision boundaries), making the classifier more robust.</p> </li> <li> <p>It generates more synthetic samples for these difficult cases and fewer for easier ones.</p> </li> <li> <p>The total number of synthetic samples is based on a desired imbalance ratio, not necessarily full balance.</p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#handling-outlier","title":"Handling Outlier","text":"<ul> <li>To handle outliers effectively, you can detect them using statistical methods like Z-score or IQR, and then choose to remove, cap, or transform them based on your analysis goals.</li> </ul>"},{"location":"ML_AI/feature_engineering/#detection-techniques","title":"\ud83d\udd0d Detection Techniques","text":"<ol> <li>Z-score Method<ul> <li>Calculates how many standard deviations a data point is from the mean.</li> <li>Typically, values with a Z-score &gt; 3 or &lt; -3 are considered outliers.</li> <li>Best for normally distributed data.</li> </ul> </li> <li>Interquartile Range (IQR)<ul> <li>Based on the spread between the 25th (Q1) and 75th (Q3) percentiles.</li> <li>Outliers fall below Q1 - 1.5\u00d7IQR or above Q3 + 1.5\u00d7IQR.</li> <li>Robust against non-normal distributions.</li> </ul> </li> <li>Isolation Forest<ul> <li>An ensemble method that isolates anomalies by randomly selecting features and split values.</li> <li>Effective for high-dimensional datasets.</li> </ul> </li> <li>Local Outlier Factor (LOF)<ul> <li>Measures the local density deviation of a data point compared to its neighbors.</li> <li>Useful for detecting outliers in clusters.</li> </ul> </li> <li>Mahalanobis Distance<ul> <li>Measures distance from the mean considering correlations between variables.</li> <li>Suitable for multivariate outlier detection.</li> </ul> </li> </ol>"},{"location":"ML_AI/feature_engineering/#handling-strategies","title":"\ud83d\udee0\ufe0f Handling Strategies","text":"<ul> <li>Remove Outliers<ul> <li>Safest when outliers are due to data entry errors or irrelevant anomalies.</li> <li>Risk: Loss of valuable information if outliers are meaningful.</li> </ul> </li> <li>Cap or Winsorize<ul> <li>Replace extreme values with a threshold (e.g., 5th and 95th percentiles).</li> <li>Preserves data size while reducing impact.</li> </ul> </li> <li>Transform Data<ul> <li>Apply log, square root, or Box-Cox transformations to reduce skewness.</li> <li>Helps normalize distributions and reduce outlier influence.</li> </ul> </li> <li>Impute Values<ul> <li>Replace outliers with mean, median, or predicted values.</li> <li>Useful when data integrity must be preserved.</li> </ul> </li> <li>Use Robust Models<ul> <li>Some algorithms (e.g., tree-based models) are less sensitive to outliers.</li> <li>Consider switching to models that handle noise better.</li> </ul> </li> </ul>"},{"location":"ML_AI/feature_engineering/#when-to-keep-outliers","title":"\u26a0\ufe0f When to Keep Outliers","text":"<p>Outliers may represent rare but important phenomena (e.g., fraud detection, medical anomalies). Always assess their context before removal.</p> Method Best For Assumptions Pros Cons Z-Score Normal distributions Data is Gaussian Simple, fast, interpretable Sensitive to mean/variance, not robust IQR Skewed or non-normal data None Robust to outliers, easy to compute Ignores multivariate structure Isolation Forest High-dimensional, large datasets None Fast, works well with many features May misclassify rare but valid points LOF Clustered or local anomalies Density-based Captures local structure, unsupervised Sensitive to <code>n_neighbors</code> parameter Mahalanobis Multivariate Gaussian data Linear relationships Accounts for feature correlation Requires invertible covariance matrix"},{"location":"ML_AI/feature_engineering/#data-encoding","title":"Data Encoding","text":"<ul> <li> <p>Data encoding transforms categorical variables into numerical formats so machine learning models can process them effectively. </p> </li> </ul>"},{"location":"ML_AI/feature_engineering/#types-of-categorical-data","title":"Types of Categorical Data","text":"<ul> <li>Binary: Two categories (e.g., Yes/No)</li> <li>Ordinal: Ordered categories (e.g., Low, Medium, High)</li> <li>Nominal: Unordered categories (e.g., Red, Blue, Green)</li> </ul>"},{"location":"ML_AI/feature_engineering/#common-encoding-techniques","title":"Common Encoding Techniques","text":"Technique Best For Description Pros Cons One-Hot Encoding Nominal Creates binary columns for each category. Preserves category independence Increases dimensionality Label Encoding Ordinal Assigns each category a unique integer. Simple, fast Imposes order where none exists Ordinal Encoding Ordinal Maps categories to ordered integers. Retains order Assumes equal spacing Target Encoding Nominal/Ordinal Replaces category with mean of target variable. Captures relationship with target Risk of data leakage Frequency Encoding Nominal Replaces category with its frequency count. Simple, preserves distribution May lose semantic meaning Binary Encoding High-cardinality Converts categories to binary digits and splits into columns. Reduces dimensionality Less interpretable"}]}